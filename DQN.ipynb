{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep reinforcement learning #\n",
    "\n",
    "Arthur ALCARAZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep, time\n",
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "import sys\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import namedtuple\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Network ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    \n",
    "    def __init__(self, observation_space_dim, action_space_dim):\n",
    "        \n",
    "        self.observation_space_dim = observation_space_dim\n",
    "        self.action_space_dim = action_space_dim\n",
    "        self._build_model()\n",
    "    \n",
    "    def _build_model(self):\n",
    "        \n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(24, input_dim=self.observation_space_dim, activation='relu'))\n",
    "        self.model.add(Dense(24, activation='relu'))\n",
    "        self.model.add(Dense(self.action_space_dim, activation='linear'))\n",
    "        self.model.compile(loss='mse', optimizer=Adam(), metrics=['mae'])\n",
    "        \n",
    "    def predict(self, next_state):\n",
    "        return self.model.predict(next_state)\n",
    "    \n",
    "    def update(self, states_batch, targets_batch):\n",
    "        return self.model.train_on_batch(states_batch, targets_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience replay ##\n",
    "\n",
    "* Transition\n",
    "* Replay Memory\n",
    "\n",
    "From Adam Paszke <https://github.com/apaszke>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def egreedy_action(state, epsilon, valid_actions, dqn):\n",
    "    \"\"\"\n",
    "    \n",
    "    Pick an action following an epsilon greedy policy\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if np.random.rand() < epsilon:\n",
    "        action = np.random.choice(valid_actions)\n",
    "    else:\n",
    "        action = np.argmax(dqn.model.predict(state))\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "env = gym.make('CartPole-v1').env\n",
    "\n",
    "observation_space_dim = env.observation_space.shape[0]\n",
    "action_space_dim = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 999/1000, loss: [407259.44, 262.05908], score: 34, total_t: 22266\n"
     ]
    }
   ],
   "source": [
    "replay_memory_size=500000\n",
    "replay_memory_init_size=50000\n",
    "update_target_network_every=1000\n",
    "discount_factor=0.99\n",
    "epsilon_start=1.0\n",
    "epsilon_end=0.1\n",
    "epsilon_decay_steps=500000\n",
    "batch_size=32\n",
    "\n",
    "policy_network = DQN(observation_space_dim, action_space_dim)\n",
    "target_network = DQN(observation_space_dim, action_space_dim)\n",
    "\n",
    "memory = ReplayMemory(50000)\n",
    "\n",
    "episodes = 1000\n",
    "\n",
    "valid_actions = [0,1]\n",
    "\n",
    "total_t = 0 \n",
    "\n",
    "# The epsilon decay schedule\n",
    "epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)\n",
    "\n",
    "stats = []\n",
    "\n",
    "# Training\n",
    "for e in range(episodes):\n",
    "    state = env.reset()\n",
    "    state = np.array([state])\n",
    "\n",
    "    for t in range(500):\n",
    "        \n",
    "        #env.render()\n",
    "        \n",
    "        # Epsilon for this time step\n",
    "        epsilon = epsilons[min(total_t, epsilon_decay_steps-1)]\n",
    "        \n",
    "        \n",
    "        action = egreedy_action(state, epsilon, valid_actions, policy_network)\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.array([next_state])\n",
    "        \n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward, done)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        # Optimize \n",
    "        if len(memory) > batch_size:\n",
    "            \n",
    "            \n",
    "            # Sample a minibatch from the replay memory\n",
    "            samples = memory.sample(batch_size)\n",
    "            \n",
    "            states_batch, action_batch, next_states_batch, reward_batch, done_batch = map(np.array, zip(*samples))\n",
    "            states_batch = states_batch.reshape((batch_size,observation_space_dim))\n",
    "            next_states_batch = next_states_batch.reshape((batch_size,observation_space_dim))\n",
    "            \n",
    "            # Calculate q values and targets\n",
    "            q_values_next = target_network.predict(next_states_batch)\n",
    "            targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * discount_factor * np.amax(q_values_next, axis=1)\n",
    "            \n",
    "            #print(states_batch.shape)\n",
    "            #print(targets_batch.shape)\n",
    "            targets = policy_network.predict(states_batch)\n",
    "            for i in range(len(targets)):\n",
    "                targets[i][action_batch[i]] = targets_batch[i]\n",
    "            \n",
    "            #print(targets.shape)\n",
    "            \n",
    "            # Perform gradient descent update\n",
    "            loss = policy_network.update(states_batch, targets)\n",
    "\n",
    "        # Maybe update the target estimator\n",
    "        if total_t % update_target_network_every == 0:\n",
    "            target_network = policy_network\n",
    "            print(\"\\nCopied model parameters to target network.\")\n",
    "        \n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            print(\"episode: {}/{}, loss: {}, score: {}, total_t: {}\".format(e, episodes, loss, t, total_t))\n",
    "            break\n",
    "        \n",
    "        total_t += 1\n",
    "        \n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Playing Game...\n"
     ]
    }
   ],
   "source": [
    "# Play game\n",
    "print(\"\\nPlaying Game...\")\n",
    "sleep(1)\n",
    "\n",
    "s = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    env.render()\n",
    "    a = np.argmax(policy_network.predict(np.array([s])))\n",
    "    newS, r, done, _ = env.step(a)\n",
    "    s = newS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
