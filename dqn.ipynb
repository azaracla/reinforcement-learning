{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "\n",
    "from time import sleep, time\n",
    "import numpy as np\n",
    "import random\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/5000, score: 19\n",
      "episode: 1/5000, score: 30\n",
      "episode: 2/5000, score: 43\n",
      "episode: 3/5000, score: 13\n",
      "episode: 4/5000, score: 23\n",
      "episode: 5/5000, score: 21\n",
      "episode: 6/5000, score: 14\n",
      "episode: 7/5000, score: 15\n",
      "episode: 8/5000, score: 10\n",
      "episode: 9/5000, score: 12\n",
      "episode: 10/5000, score: 31\n",
      "episode: 11/5000, score: 17\n",
      "episode: 12/5000, score: 19\n",
      "episode: 13/5000, score: 17\n",
      "episode: 14/5000, score: 20\n",
      "episode: 15/5000, score: 13\n",
      "episode: 16/5000, score: 10\n",
      "episode: 17/5000, score: 11\n",
      "episode: 18/5000, score: 12\n",
      "episode: 19/5000, score: 23\n",
      "episode: 20/5000, score: 12\n",
      "episode: 21/5000, score: 13\n",
      "episode: 22/5000, score: 12\n",
      "episode: 23/5000, score: 36\n",
      "episode: 24/5000, score: 41\n",
      "episode: 25/5000, score: 11\n",
      "episode: 26/5000, score: 31\n",
      "episode: 27/5000, score: 15\n",
      "episode: 28/5000, score: 8\n",
      "episode: 29/5000, score: 26\n"
     ]
    }
   ],
   "source": [
    "# Environment\n",
    "env = gym.make('CartPole-v1').env\n",
    "inputCount = env.observation_space.shape[0]\n",
    "actionsCount = env.action_space.n\n",
    "\n",
    "# Neural Network\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(24, input_dim=inputCount, activation='relu'),\n",
    "    keras.layers.Dense(24, activation='relu'),\n",
    "    keras.layers.Dense(actionsCount, activation='linear')\n",
    "])\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
    "\n",
    "model.compile(loss='mse', optimizer=tf.train.AdamOptimizer(), metrics=['mae'])\n",
    "# Load weights\n",
    "#model.load_weights(\"weights.h5\")\n",
    "\n",
    "# Hyperparameters\n",
    "gamma = 1.0\n",
    "epsilon = 1.0\n",
    "epsilonMin = 0.01\n",
    "epsilonDecay = 0.999\n",
    "episodes = 5000\n",
    "\n",
    "# Memory (Remember & Replay)\n",
    "memory = []\n",
    "batch_size = 64\n",
    "memoryMax = 50000\n",
    "\n",
    "# Training\n",
    "for e in range(episodes):\n",
    "    s = env.reset()\n",
    "    s = np.array([s])\n",
    "\n",
    "    for time in range(500):\n",
    "        # Act greedy sometimes\n",
    "        if np.random.rand() <= epsilon:\n",
    "            a = random.randrange(actionsCount)\n",
    "        else:\n",
    "            a = np.argmax(model.predict(s))\n",
    "\n",
    "        newS, r, done, _ = env.step(a)\n",
    "        newS = np.array([newS])\n",
    "        target = r + gamma * np.max(model.predict(newS))\n",
    "        target_f = model.predict(s)[0]\n",
    "        target_f[a] = target\n",
    "        model.fit(s, target_f.reshape(-1, actionsCount), epochs=1, verbose=0, callbacks=[tensorboard])\n",
    "        memory.append((s, a, r, newS, done))\n",
    "        s = newS\n",
    "\n",
    "        # free first items in memory\n",
    "        if len(memory)==memoryMax:\n",
    "            del memory[:5000]\n",
    "\n",
    "        if done:\n",
    "            print(\"episode: {}/{}, score: {}\".format(e, episodes, time))\n",
    "            break\n",
    "\n",
    "    if epsilon > epsilonMin:\n",
    "        epsilon *= epsilonDecay\n",
    "\n",
    "    # Replay memory\n",
    "    if len(memory) > batch_size:\n",
    "        minibatch = random.sample(memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "              target = reward + gamma * np.max(model.predict(next_state))\n",
    "\n",
    "            target_f = model.predict(state)[0]\n",
    "            target_f[action] = target\n",
    "            model.fit(state, target_f.reshape(-1, actionsCount), epochs=1, verbose=0)\n",
    "\n",
    "\n",
    "# Save weights\n",
    "model.save_weights(\"weights.h5\")\n",
    "\n",
    "# Play game\n",
    "print(\"\\nPlaying Game...\")\n",
    "sleep(1)\n",
    "\n",
    "s = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    env.render()\n",
    "    a = np.argmax(model.predict(np.array([s])))\n",
    "    newS, r, done, _ = env.step(a)\n",
    "    s = newS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
